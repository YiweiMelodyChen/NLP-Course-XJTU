# NLP with Deep Learning -- XJTU

assignment 1: Use word2vec and Glove to train distributed representation on the Intel computer, and compare their performance in **word clustering**. Analysis the compacts of relative corpus(like Pubmed) and general corpus(like Wikipedia) on the clustering.

[TOC]

## Introduction 

useful blog: https://zhuanlan.zhihu.com/p/56382372

differences between glove and word2vec and basic ideas about these two algorithms

## Dataset

Wikipedia dataset dump https://dumps.wikimedia.org/enwiki/latest/

I choose https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-abstract.xml.gz   about 8.2G

Pubmed dataset dump 



## Word2Vec

two model: CBOW & Skip-Gram

useful blog: https://guohongyi.com/2020/11/19/%E7%94%A8Python%E6%89%8B%E6%92%95word2vec/

useful repository: https://github.com/jind11/word2vec-on-wikipedia

### CBOW



### Skip-Gram



## Glove

followed by https://nlp.stanford.edu/projects/glove/

